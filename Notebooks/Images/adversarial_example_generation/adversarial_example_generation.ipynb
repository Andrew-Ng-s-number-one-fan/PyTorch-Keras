{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adversarial Example Generation\n",
    "\n",
    "Link: https://pytorch.org/tutorials/beginner/fgsm_tutorial.html\n",
    "\n",
    "If you are reading this, hopefully you can appreciate how effective some machine learning models are. Research is constantly pushing ML models to be faster, more accurate, and more efficient. However, an often overlooked aspect of designing and training models is security and robustness, especially in the face of an adversary who wishes to fool the model.\n",
    "\n",
    "This tutorial will raise your awareness to the security vulnerabilities of ML models, and will give insight into the hot topic of adversarial machine learning. You may be surprised to find that adding imperceptible perturbations to an image can cause drastically different model performance. Given that this is a tutorial, we will explore the topic via example on an image classifier. Specifically we will use one of the first and most popular attack methods, the **Fast Gradient Sign Attack (FGSM)**, to fool an MNIST classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Threat Model\n",
    "\n",
    "For context, there are many categories of adversarial attacks, each with a different goal and assumption of the attacker’s knowledge. However, in general the overarching goal is to add the least amount of perturbation to the input data to cause the desired misclassification. There are several kinds of assumptions of the attacker’s knowledge, two of which are: **white-box** and **black-box**. \n",
    "\n",
    "A **white-box** attack assumes the attacker has full knowledge and access to the model, including architecture, inputs, outputs, and weights. \n",
    "\n",
    "A **black-box** attack assumes the attacker only has access to the inputs and outputs of the model, and knows nothing about the underlying architecture or weights. \n",
    "\n",
    "There are also several types of goals, including **misclassification** and **source/target misclassification**. A goal of **misclassification** means the adversary only wants the output classification to be wrong but does not care what the new classification is. A source/target misclassification means the adversary wants to alter an image that is originally of a specific source class so that it is classified as a specific target class.\n",
    "\n",
    "In this case, the *FGSM* attack is a *white-box attack* with the goal of *misclassification*. With this background information, we can now discuss the attack in detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fast Gradient Sign Attack\n",
    "\n",
    "One of the first and most popular adversarial attacks to date is referred to as the **Fast Gradient Sign Attack (FGSM)** and is described by Goodfellow et. al. in [Explaining and Harnessing Adversarial Examples](https://arxiv.org/abs/1412.6572). The attack is remarkably powerful, and yet intuitive. It is designed to attack neural networks by leveraging the way they learn, **gradients**. \n",
    "\n",
    "The idea is simple, rather than working to minimize the loss by adjusting the weights based on the backpropagated gradients, the attack *adjusts the input data to maximize the loss* based on the same backpropagated gradients. In other words, the attack uses the gradient of the loss w.r.t the input data, then adjusts the input data to maximize the loss.\n",
    "\n",
    "Before we jump into the code, let’s look at the famous [FGSM](https://arxiv.org/abs/1412.6572) panda example and extract some notation.\n",
    "\n",
    "![EXAMPLE](https://pytorch.org/tutorials/_images/fgsm_panda_image.png)\n",
    "\n",
    "From the figure, x is the original input image correctly classified as a “panda”, y is the ground truth label for x, θ represents the model parameters, and J(θ,x,y) is the loss that is used to train the network. The attack backpropagates the gradient back to the input data to calculate ∇xJ(θ,x,y). Then, it adjusts the input data by a small step (ϵ or 0.007 in the picture) in the direction (i.e. sign(∇xJ(θ,x,y))) that will maximize the loss. The resulting perturbed image, x′, is then misclassified by the target network as a “gibbon” when it is still clearly a “panda”.\n",
    "\n",
    "Hopefully now the motivation for this tutorial is clear, so lets jump into the implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "In this section, we will discuss the input parameters for the tutorial, define the model under attack, then code the attack and run some tests.\n",
    "\n",
    "### Inputs\n",
    "\n",
    "There are only three inputs for this tutorial, and are defined as follows:\n",
    "\n",
    "- **epsilons** - List of epsilon values to use for the run. It is important to keep 0 in the list because it represents the model performance on the original test set. Also, intuitively we would expect the larger the epsilon, the more noticeable the perturbations but the more effective the attack in terms of degrading model accuracy. Since the data range here is [0,1], no epsilon value should exceed 1.\n",
    "- **pretrained_model** - path to the pretrained MNIST model which was trained with [pytorch/examples/mnist](https://github.com/pytorch/examples/tree/master/mnist). For simplicity, download the pretrained model [here](https://drive.google.com/drive/folders/1fn83DF14tWmit0RTKWRhPq5uVXt73e0h?usp=sharing).\n",
    "- **use_cuda** - boolean flag to use CUDA if desired and available. Note, a GPU with CUDA is not critical for this tutorial as a CPU will not take much time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epsilons = [0., 0.05, 0.10, 0.15, 0.20, 0.25, 0.30]\n",
    "pretrained_model = \"data/lenet_mnist_model.pth\"\n",
    "use_cuda=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Under Attack\n",
    "\n",
    "As mentioned, the model under attack is the same MNIST model from [pytorch/examples/mnist](https://github.com/pytorch/examples/tree/master/mnist). You may train and save your own MNIST model or you can download and use the provided model. The *Net* definition and test dataloader here have been copied from the MNIST example. The purpose of this section is to define the model and dataloader, then initialize the model and load the pretrained weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# LeNet model definition\n",
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout2d(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9920512it [00:01, 5111504.87it/s]                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/MNIST/raw/train-images-idx3-ubyte.gz to ../data/MNIST/raw\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32768it [00:00, 41606.30it/s]            \n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/MNIST/raw/train-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1654784it [00:01, 872637.05it/s]                            \n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/MNIST/raw/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8192it [00:00, 10421.61it/s]            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# MNIST test dataset and Dataloader declaration\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(\"../data\", train=False, download=True, transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "    ])),\n",
    "    batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: False\n"
     ]
    }
   ],
   "source": [
    "# Define what device we are using\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "device = torch.device(\"cuda\" if (use_cuda and torch.cuda.is_available()) else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize the netwrok\n",
    "model = Net().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=320, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the pretrained model\n",
    "model.load_state_dict(torch.load(pretrained_model, map_location='cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
       "  (fc1): Linear(in_features=320, out_features=50, bias=True)\n",
       "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the model in evaluation mode. In this case this is for the Dropout layers\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FGSM Attack\n",
    "\n",
    "Now, we can define the function that creates the adversarial examples by perturbing the original inputs. The `fgsm_attack` function takes three inputs, \n",
    "\n",
    "- *image* is the original clean image (x), \n",
    "- *epsilon* is the pixel-wise perturbation amount (ϵ), and \n",
    "- *data_grad* is gradient of the loss w.r.t the input image (∇xJ(θ,x,y)). The function then creates perturbed image as\n",
    "\n",
    "    perturbed_image=image+epsilon∗sign(data_grad)=x+ϵ∗sign(∇xJ(θ,x,y))\n",
    "    \n",
    "Finally, in order to maintain the original range of the data, the perturbed image is clipped to range [0,1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# FGSM attack code\n",
    "def fgsm_attack(image, epsilon, data_grad):\n",
    "    # Collect the element-wise sign of the data gradient\n",
    "    sign_data_grad = data_grad.sign()\n",
    "    \n",
    "    # Create the perturbed image by adjusting each pixel of the input image\n",
    "    perturbed_image = image + epsilon * sign_data_grad\n",
    "    \n",
    "    # Adding clipping to maintain [0, 1] range\n",
    "    perturbed_image = torch.clamp(perturbed_image, 0, 1)\n",
    "    \n",
    "    # Return the perturbed image\n",
    "    return perturbed_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Function\n",
    "\n",
    "Finally, the central result of this tutorial comes from the `test` function. Each call to this test function performs a full test step on the MNIST test set and reports a final accuracy. However, notice that this function also takes an *epsilon* input. This is because the `test` function reports the accuracy of a model that is under attack from an adversary with strength ϵ. More specifically, for each sample in the test set, the function computes the gradient of the loss w.r.t the input data (`data_grad`), creates a perturbed image with `fgsm_attack` (`perturbed_data`), then checks to see if the perturbed example is adversarial. In addition to testing the accuracy of the model, the function also saves and returns some successful adversarial examples to be visualized later.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test(model, device, test_loader, epsilon):\n",
    "    \n",
    "    # Accuracy counter\n",
    "    correct = 0\n",
    "    adv_examples = []\n",
    "    \n",
    "    # Loop over all examples in test set\n",
    "    for data, target in test_loader:\n",
    "        \n",
    "        # Send the data and labels to the device:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        # Set requires_grad attribute of tensor. Important for Attack\n",
    "        data.requires_grad = True\n",
    "        \n",
    "        # Forward pass the data through the model:\n",
    "        output = model(data)\n",
    "        init_pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        \n",
    "        # If the initial prediction is wrong, don't bother attacking, just move on\n",
    "        if init_pred.item() != target.item():\n",
    "            continue\n",
    "        \n",
    "        # Calculate the loss\n",
    "        loss = F.nll_loss(output, target)\n",
    "        \n",
    "        # Zero all existing gradients\n",
    "        model.zero_grad()\n",
    "        \n",
    "        # Calculate gradients of model in backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Collect datagrad\n",
    "        data_grad = data.grad.data\n",
    "        \n",
    "        # Call FGSM Attack\n",
    "        perturbed_data = fgsm_attack(data, epsilon, data_grad)\n",
    "        \n",
    "        # Re-classify the perturbed image\n",
    "        output = model(perturbed_data)\n",
    "        \n",
    "        # Check for success\n",
    "        final_pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        if final_pred.item() == target.item():\n",
    "            correct += 1\n",
    "            # Special case for saving 0 epsilon examples:\n",
    "            if (epsilon == 0) and (len(adv_examples) < 5):\n",
    "                adv_ex = perturbed_data.squeeze().detach().cpu().numpy()\n",
    "                adv_examples.append((init_pred.item(), final_pred.item(), adv_ex))\n",
    "                \n",
    "    # Calculate final accuracy for this epsilon\n",
    "    final_acc = correct / float(len(test_loader))\n",
    "    print(\"Epsilon: {}\\tTest Accuracy = {} / {} = {}\".format(epsilon, correct, len(test_loader), final_acc))\n",
    "    \n",
    "    # Return the accuracy and an adversarial example\n",
    "    return final_acc, adv_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Attack\n",
    "\n",
    "The last part of the implementation is to actually run the attack. Here, we run a full test step for each *epsilon* value in the epsilons input. For each epsilon we also save the final accuracy and some successful adversarial examples to be plotted in the coming sections. Notice how the printed accuracies decrease as the epsilon value increases. Also, note the ϵ=0 case represents the original test accuracy, with no attack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon: 0.0\tTest Accuracy = 9810 / 10000 = 0.981\n",
      "Epsilon: 0.05\tTest Accuracy = 9426 / 10000 = 0.9426\n",
      "Epsilon: 0.1\tTest Accuracy = 8510 / 10000 = 0.851\n",
      "Epsilon: 0.15\tTest Accuracy = 6826 / 10000 = 0.6826\n",
      "Epsilon: 0.2\tTest Accuracy = 4301 / 10000 = 0.4301\n",
      "Epsilon: 0.25\tTest Accuracy = 2082 / 10000 = 0.2082\n",
      "Epsilon: 0.3\tTest Accuracy = 869 / 10000 = 0.0869\n"
     ]
    }
   ],
   "source": [
    "accuracies = []\n",
    "examples = []\n",
    "\n",
    "# Run test for each epsilon\n",
    "for eps in epsilons:\n",
    "    acc, ex = test(model, device, test_loader, eps)\n",
    "    accuracies.append(acc)\n",
    "    examples.append(ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "### Accuracy vs Epsilon\n",
    "\n",
    "The first result is the accuracy versus epsilon plot. As alluded to earlier, as epsilon increases we expect the test accuracy to decrease. This is because larger epsilons mean we take a larger step in the direction that will maximize the loss. Notice the trend in the curve is not linear even though the epsilon values are linearly spaced. For example, the accuracy at ϵ=0.05 is only about 4% lower than ϵ=0, but the accuracy at ϵ=0.2 is 25% lower than ϵ=0.15. Also, notice the accuracy of the model hits random accuracy for a 10-class classifier between ϵ=0.25 and ϵ=0.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAU0AAAFNCAYAAACE8D3EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f94278f4c18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(5, 5))\n",
    "plt.plot(epsilons, accuracies, \"*-\")\n",
    "plt.yticks(np.arange(0, 1.1, step=0.1))\n",
    "plt.xticks(np.arange(0, 0.35, step=0.05))\n",
    "plt.xlabel(\"Epsilon\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Accuracy vs Epsilon\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Adversarial Examples\n",
    "\n",
    "Remember the idea of no free lunch? In this case, as epsilon increases the test accuracy decreases **BUT** the perturbations become more easily perceptible. In reality, there is a tradeoff between accuracy degredation and perceptibility that an attacker must consider. Here, we show some examples of successful adversarial examples at each epsilon value. Each row of the plot shows a different epsilon value. The first row is the ϵ=0 examples which represent the original “clean” images with no perturbation. The title of each image shows the “original classification -> adversarial classification.” Notice, the perturbations start to become evident at ϵ=0.15 and are quite evident at ϵ=0.3. However, in all cases humans are still capable of identifying the correct class despite the added noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjMAAAB3CAYAAAD2BMGRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f942785da58>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot several examples of adversarial samples at each epsilon\n",
    "cnt = 0\n",
    "plt.figure(figsize=(8, 10))\n",
    "for i in range(len(epsilons)):\n",
    "    for j in range(len(examples[i])):\n",
    "        cnt += 1\n",
    "        plt.subplot(len(epsilons), len(examples[0]), cnt)\n",
    "        plt.xticks([], [])\n",
    "        plt.yticks([], [])\n",
    "        if j == 0:\n",
    "            plt.ylabel(\"Eps: {}\".format(epsilons[i]), fontsize=14)\n",
    "        orig, adv, ex = examples[i][j]\n",
    "        plt.title(\"{} -> {}\".format(orig, adv))\n",
    "        plt.imshow(ex, cmap=\"gray\")\n",
    "        \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where to go next?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully this tutorial gives some insight into the topic of adversarial machine learning. There are many potential directions to go from here. This attack represents the very beginning of adversarial attack research and since there have been many subsequent ideas for how to attack and defend ML models from an adversary. In fact, at NIPS 2017 there was an adversarial attack and defense competition and many of the methods used in the competition are described in this paper: [Adversarial Attacks and Defences Competition](https://arxiv.org/pdf/1804.00097.pdf). The work on defense also leads into the idea of making machine learning models more *robust* in general, to both naturally perturbed and adversarially crafted inputs.\n",
    "\n",
    "Another direction to go is adversarial attacks and defense in different domains. Adversarial research is not limited to the image domain, check out [this](https://arxiv.org/pdf/1801.01944.pdf) attack on speech-to-text models. But perhaps the best way to learn more about adversarial machine learning is to get your hands dirty. Try to implement a different attack from the NIPS 2017 competition, and see how it differs from FGSM. Then, try to defend the model from your own attacks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
