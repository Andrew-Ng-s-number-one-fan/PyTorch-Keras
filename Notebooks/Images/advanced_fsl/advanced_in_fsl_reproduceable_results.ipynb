{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advances in few-shot learning: reproducing results in PyTorch\n",
    "\n",
    "![fsl](https://miro.medium.com/max/3840/1*cbsl9NSG5DXq4qSWJXLNsw.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Few-shot learning is an exciting field of machine learning which aims to close the gap between machine and human in the challenging task of learning from few examples. In my [previous post](https://medium.com/@oknagg/advances-in-few-shot-learning-a-guided-tour-36bc10a68b77) I provided a high level summary of three cutting edge papers in few-shot learning — I assume you’ve either read that, are already familiar with these papers or are in the process of reproducing them yourself.\n",
    "\n",
    "In this post I will guide you through my experience in reproducing the results of these papers on the Omniglot and miniImageNet datasets, including some of the pitfalls and stumbling blocks on the way. Each paper has its own section in which I provide a Github gist with PyTorch code to perform a single parameter update on the model described by the paper. To train the model just have to put that function inside a loop over the training data. Less interesting details such as dataset handling are omitted for brevity.\n",
    "\n",
    "Reproducibility is very important, it is the foundation of any field that claims to be scientific. This makes me believe that the prevalence of code-sharing and open-sourcing in machine learning is truly admirable. While publishing code alone is not reproducibility (as there may be implementation errors) it opens up researchers methods to public scrutiny and more importantly accelerates the research of others in the field. In light of this I’d like to thank the authors of these papers for sharing their code as well as any others who’ve open-sourced their implementations.\n",
    "\n",
    "For the full implementation please see my Github repo at https://github.com/oscarknagg/few-shot.\n",
    "\n",
    "![Things don’t always go to plan. Just see this training curve of a failed MAML implementation for example!](https://miro.medium.com/max/784/1*cVebkA3CiwpaIIUA1UNe8Q.png)\n",
    "*Things don’t always go to plan. Just see this training curve of a failed MAML implementation for example!*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets\n",
    "\n",
    "There are two image datasets on which few-shot learning algorithms are evaluated. The first is the Omniglot dataset, which contains 20 images each of roughly 1600 characters from 50 alphabets. These images are typically 28x28 grayscale which is one reason why this dataset is often called the transpose of MNIST.\n",
    "\n",
    "![Samples from the Omniglot dataset.](https://miro.medium.com/max/1400/1*T_4SiA5WB1tJ4makjdsW3Q.png)\n",
    "*Samples from the Omniglot dataset*\n",
    "\n",
    "The second is the miniImageNet dataset, a subset of ImageNet intended to be a more challenging benchmark without being as cumbersome as the full ImageNet dataset. miniImageNet consists of 60,000, 84x84 RGB images with 600 images per class.\n",
    "![Samples from the miniImageNet dataset before taking center crop and resizing to 84x84](https://miro.medium.com/max/1400/1*HL2_qYvwx_6wPrBB4RxPKA.png)\n",
    "*Samples from the miniImageNet dataset before taking center crop and resizing to 84x84*\n",
    "\n",
    "In both cases the classes in the training and validation sets are disjoint. I did not use the same training and validation splits as the original papers as my goal is not to reproduce them down to their last minute detail.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matching Networks\n",
    "\n",
    "In [Matching Networks](https://arxiv.org/pdf/1606.04080.pdf) Vinyals et al introduce the idea of a **fully differentiable nearest neighbours classifier** that is both trained and tested on few-shot tasks.\n",
    "\n",
    "The Matching Networks algorithm can be summarised as follows:\n",
    "\n",
    "- First embed all samples (query and support set) using an **encoder network** (4 layer CNN in this case). This is performed by model.encode() (line 41).\n",
    "- Optionally calculate **full context embeddings (FCE)**. An LSTM takes the original embeddings as inputs and outputs modified embeddings, taking into account the support set. This is performed by model.f() and model.g() (lines 62 and 67).\n",
    "- Calculate **pairwise distances** between query samples and support sets and normalise using softmax (lines 69 to 77).\n",
    "- Calculate **predictions** by taking the weighted average of the support set labels with the normalised distance (lines 83–89).\n",
    "\n",
    "Some things to note:\n",
    "\n",
    "- In this example the $x$ Tensor contains first the support set samples and then the query. For Omniglot it will have shape $(n_{support} + n_{query}, 1, 28, 28)$.\n",
    "- The math in the previous post is for one query sample but Matching Networks are in fact trained with a batch of query samples of size $q_{queries} * k_{way}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.utils import clip_grad_norm_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matching_net_episode(model: Module,\n",
    "                         optimizer: Optimizer,\n",
    "                         loss_fn: Loss,\n",
    "                         x: torch.Tensor,\n",
    "                         y: torch.Tensor,\n",
    "                         n_shot: int,\n",
    "                         k_way: int,\n",
    "                         q_queries: int,\n",
    "                         distance: str,\n",
    "                         fce: bool,\n",
    "                         train: bool):\n",
    "    \"\"\"\n",
    "    Performs a single training episode for a Matching Network.\n",
    "    \n",
    "    Args:\n",
    "        model: Matching Network to be trained.\n",
    "        optimizer: Optimizer to calculate gradient step from loss.\n",
    "        loss_fn: Loss function to calculate between predictions and outputs.\n",
    "        x: Input samples of few-shot classification task.\n",
    "        y: Input labels of few-shot classification task.\n",
    "        n_shot: Number of examples per class in the support set.\n",
    "        k_way: Number of classes in the few-shot classification task.\n",
    "        q_queries: Number of example per class in the query set.\n",
    "        distance: Distance metric to use when claculating distance between support and query set samples.\n",
    "        fce: Whether or not to use fully condictional embeddings.\n",
    "        train: Whether (True) or not (False) to perform a parameter update.\n",
    "        \n",
    "    Returns:\n",
    "        loss: Loss of the Matching Network on this task.\n",
    "        y_pred: Predicted class probabilities for the query set on this task.\n",
    "    \"\"\"\n",
    "    \n",
    "    if train:\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "    else:\n",
    "        model.eval()\n",
    "        \n",
    "    # Embed all samples\n",
    "    embeddings = model.encoder(x)\n",
    "    \n",
    "    # Samples are ordered by the NShotWrapper class as follows:\n",
    "    # k lots of n support samples from a particular class\n",
    "    # k lots of q query samples from those classes\n",
    "    support = embeddings[: n_shot * k_way]\n",
    "    queries = embeddings[n_shot * q_queries :]\n",
    "    y_support = y[: n_shot * k_way]\n",
    "    y_queries = y[n_shot * q_queries :]\n",
    "    \n",
    "    # Optionally apply full context embeddings (FCE):\n",
    "    if fce:\n",
    "        # LSTM requires input of shape (seq_len, batch, input_size).\n",
    "        # 'support' is of shape (k_way * n_shot, embedding_dim) and we want the LSTM to \n",
    "        # treat the support set as a sequence so add a single dimension to transform support\n",
    "        # set to the shape (k_way * n_shot, 1, embedding_dim) and then remove the batch \n",
    "        # dimension afterwards\n",
    "        \n",
    "        # Calculate the fully conditional embedding, g, for support set samples as \n",
    "        # described in appendix A.2 of the paper. \n",
    "        # g takes the form of a bidirectional LSTM with a skip connection from inputs to outputs.\n",
    "        support, _, _ = model.g(support.unsqueeze(1))\n",
    "        support = support.sequeeze(1)\n",
    "        \n",
    "        # Calculate the fully conditional embedding, f, for the query set samples\n",
    "        # as described in appendix A.1 of the paper.\n",
    "        queries = model.f(support, queries)\n",
    "    \n",
    "    # Calculate distance between all queries and all prototypes\n",
    "    # Output should have shape (q_queries * k_way, k_way) = (num_queries, k_way)\n",
    "    distances = (\n",
    "        queries.unsqueeze(1).expand(queries.shape[0], support.shape[0], -1) - \n",
    "        queries.unsqueeze(0).expand(queries.shape[0], support.shape[0], -1)\n",
    "    ).pow(2).sum(dim=2)\n",
    "    \n",
    "    # Calculate \"attention\" as softmax over support-query distances\n",
    "    attention = (-distances).softmax(dim=1)\n",
    "    \n",
    "    # Calculate predictions as in equation (1) from Matching Networks\n",
    "    # y_hat = \\sum_{i=1}^{k} a(x_hat, x_i) y_i\n",
    "    # Create one-hot encoded label vector for the support set, the \n",
    "    # default PyTorch format is for labels to be integers\n",
    "    y_onehot = torch.zeros(k * n, k)\n",
    "    \n",
    "    # Unsqueeze to force y to be 2D as this\n",
    "    # is needed for .scatter()\n",
    "    y_onehot = y_onehot.scatter(1, y_support, 1)\n",
    "    \n",
    "    y_pred = torch.mm(attention, y_onehot.cuda().double())\n",
    "    \n",
    "    # Calculated loss with negative log likelihood\n",
    "    # Clip predictions for numerical stability\n",
    "    clipped_y_pred = y_pred.clamp(1e-8, 1 - 1e-8)\n",
    "    loss = loss_fn(clipped_y_pred.log(), y_queries)\n",
    "    \n",
    "    if train:\n",
    "        # Backpropagate gradients\n",
    "        loss.backward()\n",
    "        \n",
    "        # I found training to be quite unstable so I clip the norm\n",
    "        # of the gradient to be at most 1\n",
    "        clip_grad_norm_(model.parameters(), 1)\n",
    "        \n",
    "        # Take gradient step\n",
    "        optimizer.step()\n",
    "        \n",
    "    return loss, y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I was unable to reproduce the results of this paper using cosine distance but was successful when using l2 distance. \n",
    "\n",
    "I believe this is because cosine distance is bounded between -1 and 1 which then limits the amount that the attention function (a(x^, x_i) below) can point to a particular sample in the support set. Since cosine distance is bounded a(x^, x_i) will never be close to 1! In the case of 5-way classification the maximum possible value of a(x^, x_i) is exp(1)/ (exp(1) + 4*exp(-1)) ≈ 0.65. This led to very slow convergence when using cosine distance.\n",
    "\n",
    "![eq1](https://miro.medium.com/max/756/1*Quo_tUQ2kE4v0c-y7n3RCA.png)\n",
    "\n",
    "![eq2](https://miro.medium.com/max/1400/1*KpI9WoSeoz0G3u9JesUdUQ.png)\n",
    "\n",
    "I think it’s possible to reproduce the results using cosine distance with either longer training times, better hyperparameters or a heuristic like multiplying the cosine distance by a constant factor. Seeing as the choice of distance is not key to the paper and results are very good using l2 distance I decided to spare myself that debugging effort.\n",
    "\n",
    "![result1](https://miro.medium.com/max/888/1*BQJY8y9mU-LRkNfXR-JaJw.png)\n",
    "![result2](https://miro.medium.com/max/812/1*X9wkdq19dl4hpExUmhpwdA.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prototypical Networks\n",
    "\n",
    "In [Prototypical Networks](https://arxiv.org/pdf/1703.05175.pdf) Snell et al use a compelling inductive bias motivated by the theory of Bregman divergences to achieve impressive few-shot performance.\n",
    "\n",
    "The Prototypical Network algorithm can be summarised as follows:\n",
    "\n",
    "- Embed all query and support samples (line 36).\n",
    "- Calculate class prototypes taking the mean of the embeddings of each class (line 48).\n",
    "- Predictions are a softmax over the distances between the query samples and the class prototypes (line 63).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proto_net_episode(model: Module,\n",
    "                      optimizer: Optimizer,\n",
    "                      loss_fn: Callable,\n",
    "                      x: torch.Tensor,\n",
    "                      y: torch.Tensor,\n",
    "                      n_shot: int,\n",
    "                      k_way: int,\n",
    "                      q_queries: int,\n",
    "                      distance: str,\n",
    "                      train: bool):\n",
    "    \"\"\"\n",
    "    Performs a single training episode for a Prototypical Network.\n",
    "    \n",
    "    Args:\n",
    "        model: Prototypical Network to be trained.\n",
    "        optimizer: Optimizer to calculate gradient steps.\n",
    "        loss_fn: Loss function to calculate between predictions and outputs. Should be cross-entropy.\n",
    "        x: Input samples of few-shot classification task.\n",
    "        y: Input labels of few-shot classification task.\n",
    "        n_shot: Number of examples per class in the support set.\n",
    "        k_way: Number of classes in the few shot classification task.\n",
    "        q_queries: Number of examples per class in the query set.\n",
    "        distance: Distance metric to use when calculating distance between class prototypes and queries.\n",
    "        train: Whether (True) or not (False) to perform a parameter update.\n",
    "        \n",
    "    Returns:\n",
    "        loss: Loss of the Prototypical Network on this task.\n",
    "        y_pred: Predicted class probabilities for the query set on this task.\n",
    "    \"\"\"\n",
    "    \n",
    "    if train:\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "    else:\n",
    "        model.eval()\n",
    "    \n",
    "    # Embed all samples\n",
    "    embeddings = model(x)\n",
    "    \n",
    "    # Samples are ordered by the NShotWrapper class as follows:\n",
    "    # k lots of n support samples from a particular class\n",
    "    # k lots of q query samples from those classes\n",
    "    support = embeddings[: n_shot * k_way]\n",
    "    queries = embeddings[n_shot * k_way :]\n",
    "    y_support = y[: n_shot * k_way]\n",
    "    y_queries = y[n_shot * q_queries :]\n",
    "    \n",
    "    # Reshape so the first dimension indexes by class then take the mean\n",
    "    # along that dimension to generate the \"prototypes\" for each class\n",
    "    prototypes = support.reshape(k, n, -1).mean(dim=1)\n",
    "    \n",
    "    # Calculate squared distances between all queries and all prototypes\n",
    "    # Output should have shape (q_queries * k_way, k_way) = (num_queries, k_way)\n",
    "    distances = (\n",
    "        queries.unsqueeze(1).expand(queries.shape[0], support.shape[0], -1) -\n",
    "        queries.unsqueeze(0).expand(queries.shape[0], support.shape[0], -1)\n",
    "    ).pow(2).sum(dim=2)\n",
    "    \n",
    "    # Calculate log p_{phi} (y = k | x)\n",
    "    log_p_y = (-distances).log_softmax(dim=1)\n",
    "    loss = loss_fn(log_p_y, y_queries)\n",
    "    \n",
    "    # Prediction probabilities are softmax over distances\n",
    "    y_pred = (-distances).softmax(dim=1)\n",
    "    \n",
    "    if train:\n",
    "        # Take gradient step\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    return loss, y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I found this paper delightfully easy to reproduce as the authors provided the full set of hyperparameters. Hence I was easily able to achieve the stated performance to within ~0.2% on the Omniglot benchmark and within a few % on the miniImageNet benchmark without having to perform any tuning of my own.\n",
    "\n",
    "![result1](https://miro.medium.com/max/774/1*soUZhuA4G226kxxAKSHYew.png)\n",
    "![result2](https://miro.medium.com/max/606/1*xHT4OOi1BoDxdoeLxS9S6g.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meta-Agnostic Meta-Learning (MAML)\n",
    "\n",
    "In [MAML](https://arxiv.org/pdf/1703.03400.pdf) Finn et al introduce a powerful and broadly applicable meta-learning algorithm to learn a network initialisation that can quickly adapt to new tasks. This paper was the most difficult yet most rewarding to reproduce of the three in this article.\n",
    "\n",
    "The MAML algorithm can be summarised as follows:\n",
    "\n",
    "- For each n-shot task in a meta-batch of tasks, create a new model using the weights of the base model AKA meta-learner (line 79).\n",
    "- Update the weights of the new model using the loss from the samples in the task by stochastic gradient descent (lines 81–92).\n",
    "- Calculate loss of the updated model on some more data from the same task (lines 94–97)\n",
    "- If performing 1st order MAML update the meta-learner weights with the gradient of the loss from part 3. If performing 2nd order MAML calculate the derivative of this loss with respect to the *original weights* (lines 110+).\n",
    "\n",
    "The biggest appeal of PyTorch is its autograd system. This is a piece of code-magic that records operations acting on `torch.Tensor` objects and dynamically builds the directed acyclic graph of these operations under the hood. Backpropagation is as simple as calling `.backwards()` on the final result. I had to learn a bit more about this system in order to calculate and apply parameter updates to the meta-learner, which I will now share with you.\n",
    "\n",
    "### 1st Order MAML — gradient swapping\n",
    "\n",
    "Typically when training a model in PyTorch you create an Optimizer object tied to the parameters of a particular model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "opt = Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When `opt.step()` is called the optimizer reads the gradients on the model parameters and calculates an update to those parameters. However in 1st order MAML we’ re going to calculate the gradients using one model (*the fast weights*) and apply the update to a *different model* i.e. the meta-learner.\n",
    "\n",
    "A solution to this is to use an under-utilised bit of PyTorch functionality in the form of `torch.Tensor.register_hook(hook)`. Register a hook function to a Tensor and this hook function will be called whenever a gradient with respect to this tensor is computed. For each parameter Tensor in the meta-learner I register a hook that simply replaces the gradient with the corresponding gradient on the fast weights (lines 111–129 in gist). This means that when `opt.step()` is called the gradients of the fast model will be used to update the meta-learner weights as desired.\n",
    "\n",
    "### 2nd Order MAML — autograd issues\n",
    "\n",
    "When making my first attempt at implementing MAML I instantiated a new model object (subclass of `torch.nn.Module`) and set the values of its weights equal to the meta-learner’s weights. However this makes it impossible to perform 2nd order MAML as the weights of the fast model are disconnected from the weights of the meta-learner in the eyes of `torch.autograd`. What this means is when I call `optimizer.step()` (line 140 in the gist) the autograd graph for the meta-learner weights is empty and no update is performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This didn't work, meta_learner weights remain unchanged\n",
    "meta_learner = ModelClass()\n",
    "opt = Adam(meta_learner.parameters(), lr=0.001)\n",
    "\n",
    "task_losses = []\n",
    "for x, y in meta_batch:\n",
    "    fast_model = ModelClass()\n",
    "    # torch.autograd loses reference here!\n",
    "    copy_weights(from=meta_learner, to=fast_model)\n",
    "    task_losses.append(update_with_batch(x, y))\n",
    "    \n",
    "meta_batch_loss = torch.stack(task_losses).mean()\n",
    "meta_batch_loss.backward()\n",
    "opt.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The solution to this is `functional_forward()` (line 17) which is a slightly awkward hack that manually performs the same operations (convolution, max pooling, etc…) as the model class using `torch.nn.functional`. This also means that I have to manually perform a parameter update of the fast model. The consequence of this is that `torch.autograd` knows to backpropagate gradients to the original weights of the meta-learner. This leads to a spectacularly large autograd graph.\n",
    "\n",
    "![PyTorch autograd graph for 1st order MAML (left) and 2nd order MAML (right) with one inner training step. You can probably see why 2nd order MAML has much higher memory requirements!](https://miro.medium.com/max/1400/1*4nyXG0ozncYxzuCE1EOBBg.png)\n",
    "*PyTorch autograd graph for 1st order MAML (left) and 2nd order MAML (right) with one inner training step. You can probably see why 2nd order MAML has much higher memory requirements!*\n",
    "\n",
    "However 2nd order MAML is a trickier beast than just that. When I first wrote my 2nd order MAML implementation I thought I had got everything to work miraculously on the first try. At least there were no exceptions right? Only after running a full set of Omniglot and miniImageNet experiments did I begin to doubt my work — the results were just too similar to 1st order MAML. This is typical of an unfortunate breed of silent ML bugs which don’t cause exceptions but only become visible in the final performance of a model.\n",
    "\n",
    "Hence I decided to buckle down and write a unit test that would confirm that I was truly performing a 2nd order update. Disclaimer: in the spirit of true test-driven development I should’ve written this test before running any experiments 😛.\n",
    "\n",
    "The test I decided on was to run the `meta_gradient_step` function on a dummy model and manually parse the autograd graph, counting the number of double backwards operations. This way I can be absolutely sure that I am performing a 2nd order update when desired. Conversely, I was able to test that my 1st order MAML implementation only performs a 1st order update with no double backwards operations.\n",
    "\n",
    "I finally located the bug to not applying the `create_graph` parameter in the inner training loop (line 86). I was, however, retaining the autograd graph of the loss on the query samples (line 97) but this was insufficient to perform a 2nd order update as the unrolled training graph was not created.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_grad(parameter_gradients, parameter_name):\n",
    "    \"\"\"\n",
    "    Creates a backward hook function that replaces the calculated gradient\n",
    "    with a precomputed value when .backward() is called.\n",
    "    \n",
    "    See: https://pytorch.org/docs/stable/autograd.html?highlight=hook#torch.Tensor.register_hook\n",
    "    for more info.\n",
    "    \"\"\"\n",
    "    \n",
    "    def replace_grad_(module):\n",
    "        return parameter_gradients[parameter_name]\n",
    "    \n",
    "    return replace_grad_\n",
    "\n",
    "def functional_forward(x: torch.Tensor, weights: dict):\n",
    "    \"\"\"\n",
    "    Performs a forward pass of the network using the PyTorch functional API.\n",
    "    \"\"\"\n",
    "    for block in [1, 2, 3, 4]:\n",
    "        x = functional_conv_block(x, \n",
    "                                  weights[f'conv{block}.0.weight'], \n",
    "                                  weights[f'conv{block}.0.bias'],\n",
    "                                  weights.get(f'conv{block}.1.weight'),\n",
    "                                  weights.get(f'conv{block}.1.bias'))\n",
    "    x = x.view(x.size(0), -1)\n",
    "        \n",
    "    x = F.linear(x, weights['logits.weight'], weights['logits.bias'])\n",
    "        \n",
    "    return x\n",
    "    \n",
    "def meta_gradient_step(model: Module,\n",
    "                       optimizer: Optimizer,\n",
    "                       loss_fn: Callable,\n",
    "                       x: torch.Tensor,\n",
    "                       y: torch.Tensor,\n",
    "                       n_shot: int,\n",
    "                       k_way: int,\n",
    "                       q_queries: int,\n",
    "                       order: int,\n",
    "                       inner_train_steps: int,\n",
    "                       inner_lr: float,\n",
    "                       train: bool,\n",
    "                       device: Union[str, torch.device]):\n",
    "    \"\"\"\n",
    "    Perform a gradient step on a meta-learner.\n",
    "    \n",
    "    Args:\n",
    "        model: Base model of the meta-learner being trained.\n",
    "        optimizer: Optimizer to calculate gradient step from loss.\n",
    "        loss_fn: Loss function to calculate between predictions and outputs.\n",
    "        x: Input samples for all few-shot tasks.\n",
    "        y: Input labels of all few-shot tasks.\n",
    "        n_shot: Number of examples per class in the support set of each task.\n",
    "        k_way: Number of classes in the few shot classification task of each task.\n",
    "        q_queries: Number of examples per class in the query set of each task. The query set is used to calculate\n",
    "                   meta-gradients after applying the update to.\n",
    "        order: Whether to use 1st order MAML (update meta-learner weights with gradients of the updated weights on the\n",
    "               query set) or 2nd order MAML (use 2nd order updates by differentiating through the gradients of the updated\n",
    "               weights on the query with respect to the original weights).\n",
    "        inner_train_steps: Number of gradient steps to fit the fast weights during each inner update.\n",
    "        inner_lr: Learning rate used to update the fast weights on the inner update.\n",
    "        train: Whether to update the meta-learner weights at the end of the episode.\n",
    "        device: Device on which to run computation.\n",
    "    \"\"\"\n",
    "    \n",
    "    data_shape = x.shape[2:]\n",
    "    create_graph = (True if order==2 else False) and train\n",
    "    \n",
    "    task_gradients = []\n",
    "    task_losses = []\n",
    "    task_predictions = []\n",
    "    \n",
    "    for meta_batch_examples, meta_batch_labels in zip(x, y):\n",
    "        # By construction x is a 5-D tensor of shape: (meta_batch_size, n*k + q*k, channels, width, height)\n",
    "        # Hence when we iterate over the first dimension we are iterating through the meta batches\n",
    "        # Equivalently y is a 2-D tensor of shape: (meta_batch_size, n*k + q*k, 1)\n",
    "        x_task_train = meta_batch_samples[: n_shot * k_way]\n",
    "        x_task_val = meta_batch_samples[n_shot * k_way :]\n",
    "        y_task_train = meta_batch_labels[: n_shot * k_way]\n",
    "        y_task_val = meta_batch_labels[n_shot * k_way :]\n",
    "        \n",
    "        # Create a fast model using the current meta model weights\n",
    "        fast_weights = OrderedDict(model.named_parameters())\n",
    "        \n",
    "        # Train the model for 'inner_train_steps' iterations\n",
    "        for inner_batch in range(inner_train_steps):\n",
    "            # Perform update of model weights\n",
    "            logits = model.functional_forward(x_task_train, fast_weights)\n",
    "            loss = loss_fn(logits, y_task_train)\n",
    "            gradients = torch.autograd.grad(loss, fast_weights.values(), create_graph=create_graph)\n",
    "            \n",
    "            # Update weights manually\n",
    "            fast_weights = OrderedDict(\n",
    "                (name, param - inner_lr * grad)\n",
    "                for ((name, param), grad) in zip(fast_weights.items(), gradients)\n",
    "            )\n",
    "            \n",
    "        # Do a pass of the model on the validation data from the current task\n",
    "        logits = functional_forward(x_task_val, fast_weights)\n",
    "        loss = loss_fn(logits, y_task_val)\n",
    "        loss.backward(retain_grad=True)\n",
    "        \n",
    "        # Get post-update accuracies\n",
    "        y_pred = logits.softmax(dim=1)\n",
    "        task_predictions.append(y_pred)\n",
    "        \n",
    "        # Accumulate losses and gradients\n",
    "        task_losses.append(loss)\n",
    "        gradients = torch.autograd.grad(loss, fast_weights.values(), create_graph=create_graph)\n",
    "        named_grads = {name: g for ((name, _), g) in zip(fast_weights.items(), gradients)}\n",
    "        task_gradients.append(named_grads)\n",
    "\n",
    "    if order == 1:\n",
    "        if train:\n",
    "            sum_task_gradients = {k: torch.stack([grad[k] for grad in task_gradients]).mean(dim=0)\n",
    "                                  for k in task_gradients[0].keys()}\n",
    "            hooks = []\n",
    "            for name, param in model.named_parameters():\n",
    "                hooks.append(\n",
    "                    param.register_hook(replace_grad(sum_task_gradients, name))\n",
    "                )\n",
    "\n",
    "            model.train()\n",
    "            optimiser.zero_grad()\n",
    "            # Dummy pass in order to create `loss` variable\n",
    "            # Replace dummy gradients with mean task gradients using hooks\n",
    "            logits = model(torch.zeros((k_way, ) + data_shape).to(device, dtype=torch.double))\n",
    "            loss = loss_fn(logits, create_nshot_task_label(k_way, 1).to(device))\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "\n",
    "            for h in hooks:\n",
    "                h.remove()\n",
    "\n",
    "        return torch.stack(task_losses).mean(), torch.cat(task_predictions)\n",
    "\n",
    "    elif order == 2:\n",
    "        model.train()\n",
    "        optimiser.zero_grad()\n",
    "        meta_batch_loss = torch.stack(task_losses).mean()\n",
    "\n",
    "        if train:\n",
    "            meta_batch_loss.backward()\n",
    "            optimiser.step()\n",
    "\n",
    "        return meta_batch_loss, torch.cat(task_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training time was quite long (over 24 hours for the 5-way, 5-shot miniImageNet experiment) but in the end I had fairly good success reproducing results.\n",
    "\n",
    "![result1](https://miro.medium.com/max/824/1*U5eIDnl4xRyaOLEVZtk_Pg.png)\n",
    "![result2](https://miro.medium.com/max/662/1*8VZj62bp1dMMZTsLz1NBug.png)\n",
    "\n",
    "I hope that you’ve learnt something useful from this technical deep dive. If you have any questions feel free to let me know in the comments."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
